# SWE-CARE Evaluation Pipeline Scripts

This directory contains scripts to automate the SWE-CARE evaluation pipeline.

## `run_eval_pipeline.py`

A bootstrap script that runs the complete evaluation pipeline:

1. **Generate text datasets** from collected SWE-CARE data
2. **Run LLM inference** on code review tasks  
3. **Evaluate predictions** using LLM evaluator (fixed to OpenAI o3)

### Prerequisites

Set up the required environment variables:

```bash
# Required
export OPENAI_API_KEY="your-openai-api-key"
export LLM_EVALUATOR_OPENAI_API_KEY="your-o3-evaluation-api-key"

# Optional
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export OPENAI_BASE_URL="https://your-custom-openai-endpoint"
export LLM_EVALUATOR_OPENAI_BASE_URL="https://your-custom-o3-endpoint"
```

### Usage

Basic usage with no file context:

```bash
python scripts/run_eval_pipeline.py \
    --dataset-file results/dataset/code_review_task_instances.jsonl \
    --output-dir results/pipeline_output \
    --model gpt-4o \
    --model-provider openai \
    --file-source none
```

With oracle file source and custom model args:

```bash
python scripts/run_eval_pipeline.py \
    --dataset-file results/dataset/code_review_task_instances.jsonl \
    --output-dir results/pipeline_output \
    --model claude-3-5-sonnet-20241022 \
    --model-provider anthropic \
    --model-args "temperature=0.5,max_tokens=4096" \
    --file-source oracle \
    --github-tokens "token1" "token2"
```

With BM25 retrieval:

```bash
python scripts/run_eval_pipeline.py \
    --dataset-file results/dataset/code_review_task_instances.jsonl \
    --output-dir results/pipeline_output \
    --model "models/gemini-2.5-pro" \
    --model-provider openai \
    --file-source bm25 \
    --k 10 \
    --retrieval-output-dir results/retrieval_output
```

### Arguments

**Required:**

- `--dataset-file`: Path to the input SWE-CARE dataset file
- `--output-dir`: Directory to save all pipeline outputs
- `--model`: Model name to use for inference
- `--model-provider`: Model provider (openai, anthropic, deepseek, qwen)

**Optional:**

- `--model-args`: Comma-separated model arguments (e.g., 'temperature=0.7,top_p=0.9')
- `--file-source`: Source strategy for files (none, oracle, bm25, all)
- `--k`: Maximum number of files to use (required for bm25/all)
- `--retrieval-output-dir`: Output directory for retrieval operations (required for bm25/all)
- `--github-tokens`: GitHub API token(s) for fetching data
- `--jobs`: Number of parallel jobs (default: 2)
- `--skip-existing`: Skip instances that already have predictions

### Output Structure

The script creates the following directory structure:

```
<output-dir>/
├── pipeline_config_YYYYMMDD_HHMMSS.json    # Complete pipeline configuration (timestamped)
├── pipeline_YYYYMMDD_HHMMSS.log           # Detailed execution log (timestamped)
├── code_review_text/                       # Generated text datasets
│   └── <dataset_name>__<file_source>.jsonl
│   └── <dataset_name>__<file_source>__k<N>.jsonl  # For bm25/all with k parameter
├── predictions/                            # Model predictions organized by model
│   └── <safe_model_name>/                  # Model-specific subdirectory
│       └── <dataset_name>__<safe_model_name>.jsonl
└── evaluation/                             # Evaluation results organized by model
    └── <safe_model_name>/                  # Model-specific subdirectory
        └── <dataset_name>__<safe_model_name>_report_YYYYMMDD_HHMMSS.jsonl
```

### Notes

- The script automatically handles model names with slashes (e.g., `models/gemini-2.5-pro`)
- Model predictions and evaluation results are organized in subdirectories by model name for better organization
- LLM evaluation is fixed to use OpenAI o3 model with `temperature=1`
- Use separate API keys for inference and evaluation via environment variables
- All intermediate results are saved for debugging and analysis
- The pipeline configuration and logs are timestamped for reproducibility
- Evaluation report detection ensures only reports generated by the current run are used
- Timestamps follow the format YYYYMMDD_HHMMSS for easy sorting and identification
