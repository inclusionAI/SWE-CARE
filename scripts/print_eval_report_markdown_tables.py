#!/usr/bin/env python3
"""
Print markdown tables from an evaluation report generated by `scripts/eval_report.py`.

Outputs 5 tables:
- LLM Performance Score (%) under SWE-CARE Benchmark (oracle setting)
- LLM’s Comprehensive Score (%) in Different Problem Domains (oracle setting)
- LLM’s Comprehensive Score (%) in Different Context
- LLM’s Comprehensive Score (%) in Different difficulty (oracle setting)
- LLM’s Comprehensive Score (%) in Different estimated review effort (oracle setting)
"""

from __future__ import annotations

import argparse
import json
import math
from pathlib import Path
from typing import Any, Iterable


PROBLEM_DOMAIN_ABBREV: dict[str, str] = {
    "Bug Fixes": "BF",
    "New Feature Additions": "NFA",
    "Code Refactoring / Architectural Improvement": "CA",
    "Documentation Update": "DU",
    "Documentation Updates": "DU",
    "Test Suite / CI Enhancements": "TC",
    "Performance Optimizations": "PO",
    "Security Patches / Vulnerability Fixes": "SV",
    "Dependency Updates & Environment Compatibility": "DE",
    "Dependency Updates & Env Compatibility": "DE",
    "Code Style, Linting, Formatting Fixes": "CLF",
}

PROBLEM_DOMAIN_COL_ORDER = ["BF", "NFA", "CA", "DU", "TC", "PO", "SV", "DE", "CLF"]
DIFFICULTY_ORDER = ["low", "medium", "high"]
EFFORT_ORDER = [f"effort_{i}" for i in range(1, 6)]

CONTEXT_COLUMNS: list[tuple[str, str]] = [
    ("oracle", "Oracle-based Context"),
    # ("none", "No Context"),
    ("bm25_k1", "BM25-based Context Top-1"),
    ("bm25_k3", "BM25-based Context Top-3"),
    ("bm25_k5", "BM25-based Context Top-5"),
]


def _pretty_model_name(model_name: str) -> str:
    overrides = {
        "gpt-4o": "GPT-4o",
        "gpt-5.2": "GPT-5.2",
        "glm-4.7": "GLM-4.7",
        "qwen3-235b-a22b": "Qwen3-235B-A22B",
        "deepseek-v3.2": "DeepSeek-v3.2",
        "kimi-k2-0905-preview": "Kimi-K2-0905-preview",
        "anthropic_claude-sonnet-4.5": "Claude-Sonnet-4.5",
        "google_gemini-3-pro-preview": "Gemini-3-Pro-preview",
        "inclusionai_ling-1t": "Ling-1T",
    }
    return overrides.get(model_name, model_name)


def _load_report(report_file: Path) -> dict[str, Any]:
    with open(report_file, "r") as f:
        return json.load(f)


def _percent(value: float | None) -> float:
    return float(value or 0.0) * 100.0


def _format_number(value: float) -> str:
    return f"{value:.2f}"


def _evaluator_score(
    evaluator_scores: dict[str, Any], evaluator_name: str
) -> float | None:
    for key, value in evaluator_scores.items():
        if str(key).lower() == evaluator_name.lower():
            try:
                return float(value)
            except (TypeError, ValueError):
                return None
    return None


def _model_based_score_percent(evaluator_scores: dict[str, Any]) -> float:
    llm = _evaluator_score(evaluator_scores, "LLMEvaluator")
    rm = _evaluator_score(evaluator_scores, "reward_model")
    vals = [v for v in (llm, rm) if v is not None]
    if not vals:
        return 0.0
    return _percent(sum(vals) / len(vals))


def _rule_based_score_percent(evaluator_scores: dict[str, Any]) -> float:
    return _percent(_evaluator_score(evaluator_scores, "RuleBasedEvaluator"))


def _comprehensive_score_percent(setting: dict[str, Any]) -> float:
    return _percent(setting.get("average_score"))


def _collect_models(report: dict[str, Any]) -> list[str]:
    models = list(report.get("model_results", {}).keys())

    def oracle_comprehensive(m: str) -> float:
        settings = report["model_results"][m].get("settings", {})
        oracle = settings.get("oracle", {})
        return float(oracle.get("average_score") or 0.0)

    return sorted(models, key=oracle_comprehensive, reverse=True)


def _bold_best_cells(
    numeric_rows: list[list[float]],
    numeric_col_indices: Iterable[int],
    abs_tol: float = 1e-9,
) -> set[tuple[int, int]]:
    best: set[tuple[int, int]] = set()
    for col_idx in numeric_col_indices:
        column_values = [row[col_idx] for row in numeric_rows]
        if not column_values:
            continue
        max_value = max(column_values)
        for row_idx, row in enumerate(numeric_rows):
            if math.isclose(row[col_idx], max_value, abs_tol=abs_tol):
                best.add((row_idx, col_idx))
    return best


def _render_markdown_table(headers: list[str], rows: list[list[str]]) -> str:
    lines = []
    lines.append("| " + " | ".join(headers) + " |")
    lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
    for row in rows:
        lines.append("| " + " | ".join(row) + " |")
    return "\n".join(lines)


def _table_performance(report: dict[str, Any]) -> str:
    headers = ["LLM", "Model-based Score", "Rule-based Score", "Comprehensive Score"]
    numeric_rows: list[list[float]] = []
    model_names = _collect_models(report)

    for model in model_names:
        oracle = report["model_results"][model]["settings"].get("oracle", {})
        evaluator_scores = oracle.get("evaluator_scores", {})
        numeric_rows.append(
            [
                _model_based_score_percent(evaluator_scores),
                _rule_based_score_percent(evaluator_scores),
                _comprehensive_score_percent(oracle),
            ]
        )

    best_cells = _bold_best_cells(numeric_rows, numeric_col_indices=[0, 1, 2])

    rows: list[list[str]] = []
    for row_idx, model in enumerate(model_names):
        values = numeric_rows[row_idx]
        formatted = []
        for col_idx, v in enumerate(values):
            cell = _format_number(v)
            if (row_idx, col_idx) in best_cells:
                cell = f"**{cell}**"
            formatted.append(cell)
        rows.append([_pretty_model_name(model), *formatted])

    return _render_markdown_table(headers, rows)


def _table_problem_domains(report: dict[str, Any]) -> str:
    headers = ["LLM", *PROBLEM_DOMAIN_COL_ORDER]
    numeric_rows: list[list[float]] = []
    model_names = _collect_models(report)

    for model in model_names:
        oracle = report["model_results"][model]["settings"].get("oracle", {})
        domain_scores = oracle.get("metadata_scores", {}).get("problem_domain", {})

        by_abbrev: dict[str, float] = {abbr: 0.0 for abbr in PROBLEM_DOMAIN_COL_ORDER}
        for domain_name, stats in domain_scores.items():
            abbr = PROBLEM_DOMAIN_ABBREV.get(str(domain_name))
            if abbr is None or abbr not in by_abbrev:
                continue
            by_abbrev[abbr] = _percent(stats.get("average_score"))

        numeric_rows.append([by_abbrev[a] for a in PROBLEM_DOMAIN_COL_ORDER])

    best_cells = _bold_best_cells(
        numeric_rows,
        numeric_col_indices=range(len(PROBLEM_DOMAIN_COL_ORDER)),
    )

    rows: list[list[str]] = []
    for row_idx, model in enumerate(model_names):
        formatted = []
        for col_idx, v in enumerate(numeric_rows[row_idx]):
            cell = _format_number(v)
            if (row_idx, col_idx) in best_cells:
                cell = f"**{cell}**"
            formatted.append(cell)
        rows.append([_pretty_model_name(model), *formatted])

    return _render_markdown_table(headers, rows)


def _table_context(report: dict[str, Any]) -> str:
    headers = ["LLM", *[label for _, label in CONTEXT_COLUMNS]]
    numeric_rows: list[list[float]] = []
    model_names = _collect_models(report)

    for model in model_names:
        settings = report["model_results"][model].get("settings", {})
        numeric_rows.append(
            [
                _comprehensive_score_percent(settings.get(key, {}))
                for key, _ in CONTEXT_COLUMNS
            ]
        )

    best_cells = _bold_best_cells(
        numeric_rows, numeric_col_indices=range(len(CONTEXT_COLUMNS))
    )

    rows: list[list[str]] = []
    for row_idx, model in enumerate(model_names):
        formatted = []
        for col_idx, v in enumerate(numeric_rows[row_idx]):
            cell = _format_number(v)
            if (row_idx, col_idx) in best_cells:
                cell = f"**{cell}**"
            formatted.append(cell)
        rows.append([_pretty_model_name(model), *formatted])

    return _render_markdown_table(headers, rows)


def _table_difficulty(report: dict[str, Any]) -> str:
    headers = ["LLM", "Low", "Medium", "High"]
    numeric_rows: list[list[float]] = []
    model_names = _collect_models(report)

    for model in model_names:
        oracle = report["model_results"][model]["settings"].get("oracle", {})
        difficulty_scores = oracle.get("metadata_scores", {}).get("difficulty", {})
        numeric_rows.append(
            [
                _percent(difficulty_scores.get(k, {}).get("average_score"))
                for k in DIFFICULTY_ORDER
            ]
        )

    best_cells = _bold_best_cells(
        numeric_rows, numeric_col_indices=range(len(DIFFICULTY_ORDER))
    )

    rows: list[list[str]] = []
    for row_idx, model in enumerate(model_names):
        formatted = []
        for col_idx, v in enumerate(numeric_rows[row_idx]):
            cell = _format_number(v)
            if (row_idx, col_idx) in best_cells:
                cell = f"**{cell}**"
            formatted.append(cell)
        rows.append([_pretty_model_name(model), *formatted])

    return _render_markdown_table(headers, rows)


def _table_effort(report: dict[str, Any]) -> str:
    headers = ["LLM", "Effort-1", "Effort-2", "Effort-3", "Effort-4", "Effort-5"]
    numeric_rows: list[list[float]] = []
    model_names = _collect_models(report)

    for model in model_names:
        oracle = report["model_results"][model]["settings"].get("oracle", {})
        effort_scores = oracle.get("metadata_scores", {}).get(
            "estimated_review_effort", {}
        )
        numeric_rows.append(
            [
                _percent(effort_scores.get(k, {}).get("average_score"))
                for k in EFFORT_ORDER
            ]
        )

    best_cells = _bold_best_cells(
        numeric_rows, numeric_col_indices=range(len(EFFORT_ORDER))
    )

    rows: list[list[str]] = []
    for row_idx, model in enumerate(model_names):
        formatted = []
        for col_idx, v in enumerate(numeric_rows[row_idx]):
            cell = _format_number(v)
            if (row_idx, col_idx) in best_cells:
                cell = f"**{cell}**"
            formatted.append(cell)
        rows.append([_pretty_model_name(model), *formatted])

    return _render_markdown_table(headers, rows)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Print markdown tables from an eval_report JSON file",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example:
  python scripts/print_eval_report_markdown_tables.py results/.../report_all.json
""",
    )
    parser.add_argument(
        "report_file",
        type=Path,
        help="Path to report JSON generated by scripts/eval_report.py",
    )
    args = parser.parse_args()

    report_file: Path = args.report_file
    if not report_file.exists():
        raise FileNotFoundError(f"Report file not found: {report_file}")

    report = _load_report(report_file)

    print("### LLM Performance Score (%) under SWE-CARE Benchmark (oracle)")
    print(_table_performance(report))
    print()
    print("### LLM’s Comprehensive Score (%) in Different Problem Domains (oracle)")
    print(_table_problem_domains(report))
    print()
    print("### LLM’s Comprehensive Score (%) in Different Context")
    print(_table_context(report))
    print()
    print("### LLM’s Comprehensive Score (%) in Different difficulty (oracle)")
    print(_table_difficulty(report))
    print()
    print(
        "### LLM’s Comprehensive Score (%) in Different estimated review effort (oracle)"
    )
    print(_table_effort(report))


if __name__ == "__main__":
    main()
